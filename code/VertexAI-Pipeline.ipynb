{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe00e3be-7d49-4368-b238-1d617eaf899e",
   "metadata": {},
   "source": [
    "# 00 - Environment Setup\n",
    "\n",
    "This is the notebook that sets up the GCP project for the creation of a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a3320d-7fcf-4072-9d79-db2845836c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d415d45f-c6c4-4c02-9066-f47580e81427",
   "metadata": {},
   "source": [
    "You can change the region to your own liking\n",
    "\n",
    "environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fb2a5a-5f09-4247-b942-4175069d78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = 'europe-west4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d4231-c7d9-4990-ab1d-7a0e6b3836b1",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aad6360-89c7-432e-9827-9ebbebe0a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db98946-029a-49c6-a775-4ec4b719aba8",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430352d9-e991-4f9f-9b6e-e852a58512c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs = storage.Client(project = PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e06c3-f23f-437c-898e-0c9c7d2f673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc253d6c-3100-4e98-be35-98ef39b42bb1",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Storage Bucket\n",
    "Check to see if bucket already exist and create if missing:\n",
    "- [GCS Python Client](https://cloud.google.com/python/docs/reference/storage/latest/google.cloud.storage.client.Client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2a041-75dd-40c4-9b6b-7f043874765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not gcs.lookup_bucket(BUCKET):\n",
    "    bucketDef = gcs.bucket(BUCKET)\n",
    "    bucket = gcs.create_bucket(bucketDef, project=PROJECT_ID, location=REGION)\n",
    "    print(f'Created Bucket: {gcs.lookup_bucket(BUCKET).name}')\n",
    "else:\n",
    "    bucketDef = gcs.bucket(BUCKET)\n",
    "    print(f'Bucket already exist: {bucketDef.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac6cc0-f108-443e-9447-882a5e3e8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Review the storage bucket in the console here:\\nhttps://console.cloud.google.com/storage/browser/{PROJECT_ID};tab=objects&project={PROJECT_ID}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b0fccb-478d-4f62-8d5a-ab45ef39c3e6",
   "metadata": {},
   "source": [
    "---\n",
    "<a id = 'permissions'></a>\n",
    "## Service Account & Permissions\n",
    "\n",
    "This notebook instance is running as a service account in GCP.  This service account will also be used to run other services in Vertex AI like training jobs and pipelines.  The service account will need permission to interact with object in Cloud Storage which requires the role ([roles/storage.objectAdmin](https://cloud.google.com/storage/docs/access-control/iam-roles)).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac0e02-00a7-46b7-8419-b74070e8499b",
   "metadata": {},
   "source": [
    "Get the current service account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de858f42-2bb9-44ad-a67f-21a50cb47e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505a1de-fd2f-4405-a178-ff457845d48b",
   "metadata": {},
   "source": [
    "Enable the Cloud Resource Manager API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2589c5-84e3-4aa6-a2d6-ff2d468874b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable cloudresourcemanager.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db3a697-4231-45e4-ac26-99ce68e97737",
   "metadata": {},
   "source": [
    "List the service accounts current roles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c3af1-04d6-42fd-8208-2cfe35ba06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud projects get-iam-policy $PROJECT_ID --filter=\"bindings.members:$SERVICE_ACCOUNT\" --format='table(bindings.role)' --flatten=\"bindings[].members\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493fd8a8-3dc8-475e-b06e-26fd370d5630",
   "metadata": {},
   "source": [
    "If the resulting list is missing `roles/storage.objectAdmin` or another role that contains this permission, like the basic role `roles/owner`, then it will need to be added for the service account. Use these instructions to complete this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419555c7-a4e3-4caa-a403-d1a8976c5446",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Go To IAM in the Google Cloud Console:\\nhttps://console.cloud.google.com/iam-admin/iam?orgonly=true&project={PROJECT_ID}&supportedpurview=organizationId')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d6d68-c683-4719-905e-89fa91ebe328",
   "metadata": {},
   "source": [
    "From the console link above, or by going to https:/console.cloud.google.com and navigating to \"IAM & Admin > IAM\":\n",
    "- Locate the row for the service account listed above: `<project number>-compute@developer.gserviceaccount.com`\n",
    "- Under the `inheritance` column click the pencil icon to edit roles\n",
    "- In the fly over menu, under `Assign roles` select `Add Another Role`\n",
    "- Click the `Select a role` box and type `Storage Object Admin`, then select `Storage Object Admin`\n",
    "- Click Save\n",
    "- Rerun the list of services below and verify the role has been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486aa116-4740-41c4-86ef-d58929002d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud projects get-iam-policy $PROJECT_ID --filter=\"bindings.members:$SERVICE_ACCOUNT\" --format='table(bindings.role)' --flatten=\"bindings[].members\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74358106-044e-440e-8dc2-2e7818942cb4",
   "metadata": {},
   "source": [
    "# 01 - Preparing pre-existing data\n",
    "Use Storage to load and prepare data for machine learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0b13df-cdb1-46b0-be8c-ef6208256d26",
   "metadata": {},
   "source": [
    "---\n",
    "## Source Data\n",
    "\n",
    "**Overview**\n",
    "\n",
    "We'll be making a single class image classification so we will need data which is related. Luckily one is available on kaggle.\n",
    "\n",
    "**The Data**\n",
    "\n",
    "- The data can be researched further at this [Kaggle link](https://www.kaggle.com/datasets/sanikamal/rock-paper-scissors-dataset).\n",
    "\n",
    "**Preparation of the Data**\n",
    "\n",
    "We will have to manually create a mapping on the google cloud bucket with the following:\n",
    "- /uploads/data/rock\n",
    "- /uploads/data/paper\n",
    "- /uploads/data/scissors\n",
    "\n",
    "Now manually upload the data without any additional folders to the respected folder on the bucket. We will take care of splitting the dataset into training, testing, evaluation later.\n",
    "Make sure there are only scissors in scissors folder and same goes for rock and paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea54ddb-0fdc-4ad5-89c3-54cc9a100bff",
   "metadata": {},
   "source": [
    "## Different Source Data (Optional)\n",
    "\n",
    "You can change the data used here to your own use case.\n",
    "\n",
    "Note that this is currently set up to be a single class image classification.\n",
    "With the use of AutoML we are limited to a few dataset types: https://cloud.google.com/vertex-ai/docs/datasets/overview\n",
    "\n",
    "Every dataset has a different way of setting itself up. Make sure to do the proper searching before changing this code to your own use-case.\n",
    "\n",
    "Simularly you are also limited to a few types of model training: https://cloud.google.com/vertex-ai/docs/training-overview\n",
    "Make sure to carefully read the most up to date documentation as anything on the GCP Platform is subject to frequent changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe947a-387f-48b6-abd5-48bb8975e4ff",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2b784-b752-41bc-ac15-85ccb21aa2e6",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bfdb8f-f062-477f-a6f1-1f3b7b12427f",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3cbe5a-20fa-43c2-9c9e-b7fbf6b03b6e",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac5650-0c85-4817-b5e7-22fc4a27088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from gcloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d825fdd2-b437-4be1-8dae-0dafb953caa3",
   "metadata": {},
   "source": [
    "clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91307105-332d-4df1-ab9f-84dbf0bea5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client(project = PROJECT_ID)\n",
    "bucket = client.get_bucket(PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e9e3d-fb2f-494a-bae5-2f4ba51ca0fc",
   "metadata": {},
   "source": [
    "parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e9706-a288-492e-a0ad-1a4e72f6c0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e338f751-4bbe-4210-bd76-5e92fc15c8bf",
   "metadata": {},
   "source": [
    "Load data for preparation (This will work if you have the proper file structure set up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd04e6e6-e369-4fa5-ad06-8065d3d08220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get adresses of files from GCS to CVS files\n",
    "!gcloud storage ls --recursive gs://$PROJECT_ID/uploads/data/rock/** > rock.csv\n",
    "!gcloud storage ls --recursive gs://$PROJECT_ID/uploads/data/paper/** > paper.csv\n",
    "!gcloud storage ls --recursive gs://$PROJECT_ID/uploads/data/scissors/** > scissors.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294fda21-da68-4695-a0db-1c44f328c9ca",
   "metadata": {},
   "source": [
    "convert item into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44270e31-71de-44f5-b03b-7a6bd5b0a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames from CSV files\n",
    "df_rock = pd.read_csv('rock.csv', header=None)\n",
    "df_paper = pd.read_csv('paper.csv', header=None)\n",
    "df_scissors = pd.read_csv('scissors.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedbf81-a148-40c5-b2f2-40df3239ffe0",
   "metadata": {},
   "source": [
    "add labels so the dataset knows which is what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94931b8-4c7b-4b1e-8d5b-a1d27367fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels\n",
    "df_rock['label'] = 'rock'\n",
    "df_paper['label'] = 'paper'\n",
    "df_scissors['label'] = 'scissors'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0442bdf0-351a-4c84-8990-98951de3dcdf",
   "metadata": {},
   "source": [
    "combine into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f59e66a-7dd7-4096-b85b-3d3c4e6a4073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all 3 DataFrames\n",
    "df_full = pd.concat([df_rock, df_paper, df_scissors])\n",
    "#df_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9243ee7f-3476-4af2-b5fd-c22b40b99f22",
   "metadata": {},
   "source": [
    "Create CSV input file and upload it to the given path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fc5e9c-fb3d-4912-9724-72d009e55ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save input file locally\n",
    "df_full.to_csv('input_file.csv', index=None, header=None)\n",
    "\n",
    "# Export input file to Cloud Storage\n",
    "blob = bucket.blob('rps/input_file.csv')\n",
    "blob.upload_from_filename('input_file.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e190b4d2-ef2a-491e-a2e1-0d41b53602f8",
   "metadata": {},
   "source": [
    "## 02 - Creating the pipeline\n",
    "\n",
    "We will be making a pipeline with the use of Kubeflow Pipelines. This is very commonly used for the creation of pipelines in GCP. We will be taking care of each step being:\n",
    "\n",
    "- Make a new dataset\n",
    "- Train the model on the dataset\n",
    "- Make an endpoint to deploy the model to\n",
    "- Deploy the model to the endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a04ce-f993-4588-a1dc-980df38e8c1f",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33825739-920d-4d23-814a-756e70cca12d",
   "metadata": {},
   "source": [
    "inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70109baf-7223-432c-89aa-392d04427343",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd4f20-4cdd-45bc-993e-3ee776fcdb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = !gcloud config list --format='value(core.account)' \n",
    "SERVICE_ACCOUNT = SERVICE_ACCOUNT[0]\n",
    "SERVICE_ACCOUNT\n",
    "\n",
    "BUCKET_URI = f\"gs://{PROJECT_ID}\"\n",
    "PIPELINE_ROOT = f\"gs://{PROJECT_ID}/rps/pipeline_root\"\n",
    "VERSION = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9dcf7a-9945-4d2b-a2ff-158694e63044",
   "metadata": {},
   "source": [
    "packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9265a8b-2488-4d5b-aa91-03b9e9ff0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "import os\n",
    "from typing import Any, Dict, List\n",
    "import kfp\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8388f270-3d48-4f89-9f63-0e4353d6388f",
   "metadata": {},
   "source": [
    "aiplatform client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a9565-23a7-4b7e-a52e-60648c2b395d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aip.init(project=PROJECT_ID, location=REGION, staging_bucket=PIPELINE_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f093fbe4-e078-4e5b-aba3-10a37ecf3ac7",
   "metadata": {},
   "source": [
    "defining the Kubeflow Pipeline which has all the listed steps mentioned above:\n",
    "\n",
    "- Make a new dataset\n",
    "- Train the model on the dataset\n",
    "- Make an endpoint to deploy the model to\n",
    "- Deploy the model to the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b30ef5-e0c9-4e42-b10f-60228ab07050",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(name=f\"rps-automl-image-training-pipeline-v{VERSION}\")\n",
    "def pipeline(project: str = PROJECT_ID, region: str = REGION, BUCKETURI: str = BUCKET_URI, version: str = VERSION):\n",
    "    from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "    from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,\n",
    "                                                              ModelDeployOp)\n",
    "    \n",
    "    ds_op = gcc_aip.ImageDatasetCreateOp(\n",
    "        project=project,\n",
    "        display_name=f\"rps-dataset-v{version}\",\n",
    "        location=region,\n",
    "        gcs_source=f\"{BUCKET_URI}/rps/input_file.csv\",\n",
    "        import_schema_uri=aip.schema.dataset.ioformat.image.single_label_classification,\n",
    "    )\n",
    "    \n",
    "    training_job_run_op = gcc_aip.AutoMLImageTrainingJobRunOp(\n",
    "        project=project,\n",
    "        display_name=f\"train-automl-rps-v{version}\",\n",
    "        prediction_type=\"classification\",\n",
    "        location=region,\n",
    "        model_type=\"CLOUD\",\n",
    "        dataset=ds_op.outputs[\"dataset\"],\n",
    "        model_display_name=f\"automl-rps-v{version}\",\n",
    "        training_fraction_split=0.7,\n",
    "        validation_fraction_split=0.15,\n",
    "        test_fraction_split=0.15,\n",
    "        budget_milli_node_hours=8000,\n",
    "    )\n",
    "\n",
    "    endpoint_op = EndpointCreateOp(\n",
    "        project=project,\n",
    "        location=region,\n",
    "        display_name=f\"endpoint-automl-rps-v{version}\",\n",
    "    )\n",
    "\n",
    "    ModelDeployOp(\n",
    "        model=training_job_run_op.outputs[\"model\"],\n",
    "        endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "        automatic_resources_min_replica_count=1,\n",
    "        automatic_resources_max_replica_count=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa070ff5-013e-4955-bdde-e0902f1f6a71",
   "metadata": {},
   "source": [
    "Compile the just defined pipeline into a json format which will appear locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ac75fc-de8c-4e4c-a013-3f7b19470703",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path=\"rps_single_image_classification_pipeline.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec3f158-da5c-4cb0-bfa5-cd59136e84cf",
   "metadata": {},
   "source": [
    "## Running the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a69f5-fa47-4e01-8468-66d891876c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aip.PipelineJob(\n",
    "    display_name='rps-pipeline',\n",
    "    template_path=\"rps_single_image_classification_pipeline.json\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add3b9d-cc4d-4160-82f3-eca235e0dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1f9181-142b-4e53-aaf4-9d8a5166b665",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm rps_single_image_classification_pipeline.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63ac7ec-73bd-46e0-ab59-9c44d4c9aefd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cleaning up (Optional)\n",
    "To clean up all Google Cloud resources used in this project, you can delete the Google Cloud project.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279fc47-7536-4b96-a18a-15e651c455d6",
   "metadata": {},
   "source": [
    "# Get resources from the pipeline to clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226ccf8e-5a07-420e-ac24-8668f51a62c2",
   "metadata": {},
   "source": [
    "Function to get details of a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68141cea-c794-4c97-b840-aced34d214ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_detail(task_details: List[Dict[str, Any]], task_name: str) -> List[Dict[str, Any]]:\n",
    "    for task_detail in task_details:\n",
    "        if task_detail.task_name == task_name:\n",
    "            return task_detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780309a6-8b33-4c3c-a37a-43f93f39db0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_task_details = (\n",
    "    job.gca_resource.job_detail.task_details\n",
    ")  # fetch pipeline task details\n",
    "\n",
    "\n",
    "# fetch endpoint from pipeline and delete the endpoint\n",
    "endpoint_task = get_task_detail(pipeline_task_details, f\"endpoint-automl-rps-v{VERSION}\")\n",
    "endpoint_resourceName = (\n",
    "    endpoint_task.outputs[\"endpoint\"].artifacts[0].metadata[\"resourceName\"]\n",
    ")\n",
    "endpoint = aip.Endpoint(endpoint_resourceName)\n",
    "# undeploy model from endpoint\n",
    "endpoint.undeploy_all()\n",
    "endpoint.delete()\n",
    "\n",
    "# fetch model from pipeline and delete the model\n",
    "model_task = get_task_detail(pipeline_task_details, f\"automl-rps-v{VERSION}\")\n",
    "model_resourceName = model_task.outputs[\"model\"].artifacts[0].metadata[\"resourceName\"]\n",
    "model = aip.Model(model_resourceName)\n",
    "model.delete()\n",
    "\n",
    "\n",
    "# fetch dataset from pipeline and delete the dataset\n",
    "dataset_task = get_task_detail(pipeline_task_details, f\"rps-dataset-v{VERSION}\")\n",
    "dataset_resourceName = (\n",
    "    dataset_task.outputs[\"dataset\"].artifacts[0].metadata[\"resourceName\"]\n",
    ")\n",
    "dataset = aip.ImageDataset(dataset_resourceName)\n",
    "dataset.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f6e4d-1828-4671-bcf2-fa5a973a53f8",
   "metadata": {},
   "source": [
    "Everything is deleted except for the Google Cloud Bucket. You can empty the whole bucket using the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6644a88-ae5d-4c4f-961c-16f56f67ef5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee8f4f8-e1eb-43e6-a932-5e68e791862e",
   "metadata": {},
   "source": [
    "# Google Functions (Optional)\n",
    "\n",
    "With addition to this I have made an endpoint which would update the input_file.csv file with each image upload.\n",
    "To launch this you will have to go [Google Cloud Functions](https://cloud.google.com/functions). \n",
    "\n",
    "There enable the API and make a new function.\n",
    "\n",
    "specs:\n",
    "- 1st gen\n",
    "- function name (your preference)\n",
    "- region (your preference)\n",
    "- httptrigger http\n",
    "- You can choose how you want to do autenthication.\n",
    "- Enable HTTPS\n",
    "- ** Hit next **\n",
    "- Select Python 3.9\n",
    "- Entrypoint (your preference but make sure it matches the function inside)\n",
    "\n",
    "Now in the main.py insert the following code:\n",
    "```py\n",
    "from google.cloud import storage\n",
    "import uuid\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "def hello_world(request):\n",
    "    if request.method=='POST':\n",
    "        \"\"\"Process the uploaded file and upload it to Google Cloud Storage.\"\"\"\n",
    "        uploaded_file = request.files.get('image')\n",
    "\n",
    "        #Check if file is in request\n",
    "        if not uploaded_file:\n",
    "            return 'No file uploaded.', 400\n",
    "\n",
    "        #Parse type\n",
    "        type_of_image = request.form.get('type')\n",
    "        if type_of_image not in ['rock', 'paper', 'scissors']:\n",
    "            return 'Invalid type', 400\n",
    "            \n",
    "        BUCKET_NAME = BUCKETNAME\n",
    "        extension = (uploaded_file.filename.split('.'))[-1:][0]\n",
    "        \n",
    "        if extension != \"png\":\n",
    "            return 'Invalid image type', 400\n",
    "\n",
    "        # Create a Cloud Storage client.\n",
    "        gcs = storage.Client()\n",
    "\n",
    "        # Get the bucket that the file will be uploaded to.\n",
    "        bucket = gcs.get_bucket(BUCKET_NAME)\n",
    "\n",
    "        new_file_name = f\"{type_of_image}-{str(uuid.uuid4())}\"\n",
    "        \n",
    "        # Create a new blob and upload the file's content.\n",
    "        #print(f\"uploads/data/{type_of_image}/\" + new_file_name)\n",
    "        blob = bucket.blob(f\"uploads/data/{type_of_image}/\" + new_file_name + f\".{extension}\")\n",
    "\n",
    "        blob.upload_from_string(\n",
    "            uploaded_file.read(),\n",
    "            content_type=uploaded_file.content_type\n",
    "        )\n",
    "\n",
    "        #Manipulate existing csv with new data paths for dataset\n",
    "        dataset_import_csv = 'rps/input_file.csv'\n",
    "        blob_csv = bucket.blob(dataset_import_csv)\n",
    "        csv_string = blob_csv.download_as_string().decode(\"utf-8\") \n",
    "        new_data_row = f\"gs://{BUCKET_NAME}/uploads/data/{type_of_image}/{new_file_name}.{extension},{type_of_image}\\n\"\n",
    "\n",
    "        print(csv_string)\n",
    "        print(type(csv_string))\n",
    "        csv_string += new_data_row\n",
    "        blob_csv.upload_from_string(\n",
    "            csv_string,\n",
    "            'text/csv'\n",
    "        )\n",
    "\n",
    "        # The public URL can be used to directly access the uploaded file via HTTP.\n",
    "        return f\"Uploaded {type_of_image}\"\n",
    "    else:\n",
    "        return \"Allowed methods: [POST]\", 400\n",
    "```\n",
    "\n",
    "make sure to adjust the `BUCKET_NAME = BUCKET NAME` to whatever your bucket name is. in this case it will be the name of your project you are running this in.\n",
    "\n",
    "and in the requirements.txt make sure to not forget the following:\n",
    "```py\n",
    "# Function dependencies, for example:\n",
    "# package>=version\n",
    "google-cloud-storage==2.7.0\n",
    "uuid\n",
    "pandas\n",
    "```\n",
    "\n",
    "Once this is done. You can deploy the functions. Once deployed you can use the endpoint to add new data to the input.csv through a post."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595dd594-0c51-4b51-aa6a-c109e500dd3f",
   "metadata": {},
   "source": [
    "# Using endpoint in Python code (Optional)\n",
    "\n",
    "To make it easy, go to your created endpoint where your model is currently launched.\n",
    "\n",
    "When in the endpoint you'll see a button to see a sample request. Make sure to choose the python tab.\n",
    "\n",
    "You can follow the instructions there.\n",
    "\n",
    "# Incase you need additional directions to use your endpoint in python:\n",
    "\n",
    "Go to: https://github.com/googleapis/python-aiplatform/blob/main/samples/snippets/prediction_service/predict_image_classification_sample.py\n",
    "\n",
    "Copy this into your python file.\n",
    "You will also need to copy the 3rd step code and paste that into your python file\n",
    "You will need credentials to use the endpoint. To create this follow the following: IAM & Admin > Service Accounts > Keys > Add Key (JSON). Make sure you put the json file in the project.\n",
    "\n",
    "Now `import os` into your project and setup the environment variables to use the function.\n",
    "`os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'PATH TO JSON CREDENTIALS'`\n",
    "\n",
    "Make sure the picture you will test with is 300x300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aad1fc-de6d-4e05-a985-1953e7f5417d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-3.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-3:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
